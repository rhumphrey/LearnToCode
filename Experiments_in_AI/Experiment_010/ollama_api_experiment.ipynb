{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bdb65f-11ef-4fea-9653-f8b21d46c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def explore_ollama_api():\n",
    "    \"\"\"\n",
    "    Explores the Ollama API endpoints using the ollama Python library.\n",
    "    This function demonstrates how to (and also the 'response' to expect) for:\n",
    "    1.  Listing available models.\n",
    "    2.  Generating a response from a model.\n",
    "    3.  Showing the embedding.\n",
    "    4.  Pulling a model (existing or new).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. List available models\n",
    "        print(\"\\n--- 1. List Available Models ---\")\n",
    "        list_response = ollama.list()\n",
    "        print(f\"\\nList Response:\\n{list_response}\")\n",
    "       \n",
    "        # Extract model names\n",
    "        model_names = [model['model'] for model in list_response['models']]\n",
    "        print(f\"\\nAvailable Models:\\n{model_names}\")\n",
    "        if not model_names:\n",
    "            print(\"No models found. Please ensure Ollama is running and has models available.\")\n",
    "            return\n",
    "\n",
    "        # 2. Generate a response from a model\n",
    "        print(\"\\n--- 2. Generate a Response ---\")\n",
    "        model_name = model_names[0]  # Use the first available model, you could choose something else\n",
    "        prompt = \"What is the capital of France?\"\n",
    "        try:\n",
    "            generate_response = ollama.generate(model=model_name, prompt=prompt)\n",
    "            print(f\"\\nGenerate Response:\\n{generate_response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            print(f\"Please check if the model {model_name} is available and Ollama is running correctly.\")\n",
    "            return\n",
    "\n",
    "        # Extract the generated response text\n",
    "        generated_text = generate_response.get('response', '')\n",
    "        print(f\"\\nGenerated Text:\\n{generated_text}\")\n",
    "\n",
    "        # 3. Show embedding\n",
    "        # What are Embeddings?\n",
    "        # Embeddings are numerical vector representations of data (words, images, etc.), translating it into a machine-understandable format. These vectors capture the semantic meaning and relationships within the data.\n",
    "        # Why are Embeddings Important?\n",
    "        # - Enable machine learning models to process complex data.\n",
    "        # - Preserve relationships for tasks like similarity search, recommendations, and natural language processing.\n",
    "        # - Reduce dimensionality for more efficient computation.\n",
    "        # How are Embeddings Created?\n",
    "        # - Embeddings are learned by models, such as neural networks trained to predict surrounding words. Techniques include Word2Vec, GloVe, and transformer models (like BERT, GPT).\n",
    "        # In Ollama:\n",
    "        # - Ollama uses embeddings to convert input text into vectors for tasks like comparing text similarity.\n",
    "        \n",
    "        print(\"\\n--- 3.  Show embedding ---\")\n",
    "        try:\n",
    "            embedding_response = ollama.embeddings(model=model_name, prompt=prompt)\n",
    "            print(f\"\\nEmbedding Response:\\n{embedding_response}\")\n",
    "            embedding_values = embedding_response.get('embedding', [])  # Get the embedding vector\n",
    "            print(f\"\\nEmbeddings: {embedding_values[:5]}... (first 5 values)\")  # Print only the first 5 values\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embeddings: {e}\")\n",
    "\n",
    "        # 4. Pull a model (check if the first model is already installed, if not, try to pull it)\n",
    "        print(\"\\n--- 4. Pull a Model (if not already present) ---\")\n",
    "        model_choice = model_names[0]      # pull with a model already installed\n",
    "        # model_choice = \"smollm:135m\"     # pull with a model not installed - pick a smaller one you know is not installed\n",
    "        # model_choice = \"tinyllama:1.1b\"  # pull with a model not installed - pick a smaller one you know is not installed \n",
    "        # model_choice = \"qwen3:0.6b\"      # pull with a model not installed - pick a smaller one you know is not installed \n",
    "        \n",
    "        try:\n",
    "            # Check if the model exists locally before attempting to pull it.\n",
    "            model_list_response = ollama.list()\n",
    "            model_names = [model['model'] for model in model_list_response['models']]\n",
    "            if model_choice not in model_names:\n",
    "                print(f\"Model '{model_choice}' not found locally. Attempting to pull...\")\n",
    "                pull_response = ollama.pull(model_choice) # pull the model\n",
    "                print(f\"\\nPull Response:\\n{pull_response}\")\n",
    "                print(f\"Model '{model_choice}' pulled successfully (or already in progress).\")\n",
    "            else:\n",
    "                print(f\"Model '{model_choice}' is already available locally.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            print(\"Please check your Ollama installation and network connection.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please ensure Ollama is running and accessible.\")\n",
    "\n",
    "# Call the experiment function\n",
    "explore_ollama_api()\n",
    "\n",
    "print(\"\\n--- That's all Folks ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26891f74-56c9-4eac-a5ee-390c29c51b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def analyze_sentiment(model_name, text):\n",
    "    \"\"\"Sends a prompt to the LLM to classify the sentiment of a text.\"\"\"\n",
    "    prompt = f\"Classify the sentiment of the following text as positive, negative, or neutral: '{text}'.\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip().lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment for text '{text}': {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_detailed_sentiment(model_name, text):\n",
    "    prompt = f\"Classify the sentiment of the following text with enhanced detail (e.g., very positive, slightly negative, neutral): '{text}'.\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip().lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing detailed sentiment for text '{text}': {e}\")\n",
    "        return None\n",
    "\n",
    "models = [\"tinyllama:1.1b\" , \"qwen3:0.6b\" ] # Choose a models available in your Ollama instance\n",
    "\n",
    "for model_to_use in models:    \n",
    "\n",
    "    text_snippets = [\n",
    "    \"This is a fantastic product! I love it.\",\n",
    "    \"I am extremely disappointed with the service.\",\n",
    "    \"The weather today is just okay.\",\n",
    "    \"This movie was absolutely amazing and captivating.\",\n",
    "    \"I felt quite frustrated by the end of the meeting.\",\n",
    "    \"The new software update seems to work as expected.\"\n",
    "    ]\n",
    "\n",
    "    print(\"--- Sentiment Analysis using Ollama API ---\")\n",
    "    print(f\"Using model: {model_to_use}\\n\")\n",
    "\n",
    "    for snippet in text_snippets:\n",
    "        sentiment = analyze_sentiment(model_to_use, snippet)\n",
    "        if sentiment:\n",
    "            print(f\"Text: '{snippet}'\")\n",
    "            print(f\"Sentiment: {sentiment}\\n\")\n",
    "\n",
    "    print(\"\\n--- Detailed Sentiment Analysis using Ollama API ---\")\n",
    "    for snippet in text_snippets:\n",
    "        detailed_sentiment = analyze_detailed_sentiment(model_to_use, snippet)\n",
    "        if detailed_sentiment:\n",
    "            print(f\"Text: '{snippet}'\")\n",
    "            print(f\"Detailed Sentiment: {detailed_sentiment}\\n\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(\"\\n--- Observation ---\")\n",
    "print(\"This demonstrates how to programmatically interact with the local LLM served by Ollama.\")\n",
    "print(\"You can now expand upon this to automate more complex tasks by sending different prompts and parsing the responses accordingly.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14fe95-0968-477a-90a6-bdf847395488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
