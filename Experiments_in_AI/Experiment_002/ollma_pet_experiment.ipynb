{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c759eb-5c6f-4cfc-bdc5-727aba799341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972ba6a-0aff-4175-92e5-8a02818cc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_model_if_needed(model_name):\n",
    "    \"\"\"\n",
    "    Attempts to pull the model. Ollama handles checking if it exists.\n",
    "    Returns True on success, raises an exception on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ollama.pull(model=model_name)\n",
    "        print(f\"Model '{model_name}' pulled successfully/already existed.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error pulling model '{model_name}': {e}\"\n",
    "        print(error_message)\n",
    "        raise  # Re-raise the exception for the caller to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899a812-0a8b-444c-8926-185c4352d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_prompt(model, prompt, prompt_type):\n",
    "    \"\"\"Internal helper function to run a prompt.\"\"\"\n",
    "    print(f\"--- {prompt_type} Prompting ---\")\n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt,\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "        result = response['message']['content'].strip()\n",
    "        print(f\"Response:\\n{result}\\n\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {prompt_type.lower()} prompting: {e}\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9390a1-cc49-4491-b810-26a7ec5f1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zero_shot(model, prompt):\n",
    "    \"\"\"Runs zero-shot prompting on the specified model.\"\"\"\n",
    "    return _run_prompt(model, prompt, \"Zero-shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3222a9-7741-4f8b-860a-10f3a5b3584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chain_of_thought(model, prompt):\n",
    "    \"\"\"Runs chain-of-thought prompting on the specified model.\"\"\"\n",
    "    return _run_prompt(model, prompt, \"Chain-of-Thought\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a90afc-26c6-4689-a8ba-7ad73280188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_few_shot(model, prompt, examples):\n",
    "    \"\"\"Runs few-shot prompting on the specified model and displays examples.\"\"\"\n",
    "    print(\"--- Few-shot Prompting ---\")\n",
    "    print(\"Examples:\")\n",
    "    for example in examples:\n",
    "        print(f\"  Prompt: {example['prompt']}\")\n",
    "        print(f\"  Completion: {example['completion']}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "\n",
    "    messages = []\n",
    "    for example in examples:\n",
    "        messages.append({'role': 'user', 'content': example['prompt']})\n",
    "        messages.append({'role': 'assistant', 'content': example['completion']})\n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        result = response['message']['content'].strip()\n",
    "        print(f\"Response:\\n{result}\\n\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error during few-shot prompting: {e}\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1969f0-dad8-47af-ab88-35e200d957ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ollama_experiment(model, task_description, zero_shot_prompt, few_shot_examples=None, chain_of_thought_prompt=None):\n",
    "    \"\"\"\n",
    "    Facilitates an experiment with different prompting techniques on an Ollama model.\n",
    "\n",
    "    Args:\n",
    "        model (str): The name of the Ollama model to use.\n",
    "        task_description (str): A brief description of the task.\n",
    "        zero_shot_prompt (str): The prompt for the zero-shot experiment.\n",
    "        few_shot_examples (list, optional): A list of dictionaries, where each dictionary\n",
    "                                            has 'prompt' and 'completion' keys representing\n",
    "                                            an example. Defaults to None.\n",
    "        chain_of_thought_prompt (str, optional): The prompt for the chain-of-thought\n",
    "                                                experiment. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of each prompting technique.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n--- Experimenting with model: {model} ---\\nTask: {task_description}\\n\")\n",
    "\n",
    "    results = {\n",
    "        \"task_description\": task_description,\n",
    "        \"model\": model,\n",
    "        \"zero_shot\": run_zero_shot(model, zero_shot_prompt),\n",
    "        \"few_shot\": run_few_shot(model, zero_shot_prompt, few_shot_examples) if few_shot_examples else None,\n",
    "        \"chain_of_thought\": run_chain_of_thought(model, chain_of_thought_prompt) if chain_of_thought_prompt else None,\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efad3c2-7fb0-4834-a2c1-449d379aea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma2\"  # Replace with your desired default model\n",
    "\n",
    "print(\"Starting experiment...pulling models as required...\")\n",
    "try:\n",
    "    pull_model_if_needed(model_name)\n",
    "    model_to_use = model_name\n",
    "    print(f\"\\nUsing model: {model_to_use}\\n\")\n",
    "\n",
    "    # Task: Summarize a paragraph of text\n",
    "    summary_task_description = \"Summarize a paragraph of text.\"\n",
    "    paragraph_to_summarize = \"\"\"\n",
    "    The quick brown rabbit jumps over the lazy frogs with no effort. The frogs,\n",
    "    startled by the sudden movement, croak loudly in protest. The rabbit,\n",
    "    unfazed by the noise, continues its journey through the lush green meadow,\n",
    "    eager to reach its burrow before sunset. The gentle breeze rustles the leaves\n",
    "    in the nearby trees, creating a soothing natural symphony.\n",
    "    \"\"\"\n",
    "    summary_zero_shot_prompt = f\"Summarize the following paragraph:\\n\\n{paragraph_to_summarize}\"\n",
    "    summary_few_shot_examples = [\n",
    "        {\n",
    "            \"prompt\": \"Summarize: The cat sat on the mat.\",\n",
    "            \"completion\": \"A cat is sitting on a mat.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Summarize: The dog barked loudly at the mailman.\",\n",
    "            \"completion\": \"A dog barked at the person delivering mail.\"\n",
    "        }\n",
    "    ]\n",
    "    summary_chain_of_thought_prompt = f\"Let's think step-by-step. First, I will identify the main entities and actions in the paragraph. Then, I will condense this information into a shorter summary. Summarize the following paragraph:\\n\\n{paragraph_to_summarize}\"\n",
    "\n",
    "    summary_results = run_ollama_experiment(\n",
    "        model=model_to_use,\n",
    "        task_description=summary_task_description,\n",
    "        zero_shot_prompt=summary_zero_shot_prompt,\n",
    "        few_shot_examples=summary_few_shot_examples,\n",
    "        chain_of_thought_prompt=summary_chain_of_thought_prompt\n",
    "    )\n",
    "\n",
    "    # Task: Solve a simple logic puzzle\n",
    "    logic_task_description = \"Solve a simple logic puzzle.\"\n",
    "    logic_puzzle = \"\"\"\n",
    "    Alice, Bob, and Carol are sitting in a row. Bob is not at either end.\n",
    "    Alice is to the left of Bob. Who is sitting on the right end?\n",
    "    \"\"\"\n",
    "    logic_zero_shot_prompt = f\"Solve this logic puzzle:\\n\\n{logic_puzzle}\"\n",
    "    logic_few_shot_examples = [\n",
    "        {\n",
    "            \"prompt\": \"Solve: If all birds can fly and a penguin is a bird, can a penguin fly?\",\n",
    "            \"completion\": \"Let's think step-by-step. All birds can fly. A penguin is a bird. Therefore, a penguin can fly. Final Answer: Yes.\"\n",
    "        }\n",
    "    ]\n",
    "    logic_chain_of_thought_prompt = f\"Let's think step-by-step. I will analyze the clues provided. Based on these clues, I can deduce the order. Solve this logic puzzle:\\n\\n{logic_puzzle}\"\n",
    "\n",
    "    logic_results = run_ollama_experiment(\n",
    "        model=model_to_use,\n",
    "        task_description=logic_task_description,\n",
    "        zero_shot_prompt=logic_zero_shot_prompt,\n",
    "        few_shot_examples=logic_few_shot_examples,\n",
    "        chain_of_thought_prompt=logic_chain_of_thought_prompt\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Experiment Finished ---\")\n",
    "\n",
    "    print(\"\\n--- Results from Experiments ---\")\n",
    "    print(summary_results)\n",
    "    print(logic_results)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Exiting due to an error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e10c36-d2eb-497a-a2ac-d102291b5e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6896ab-4c83-4a32-9dee-376909061952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
