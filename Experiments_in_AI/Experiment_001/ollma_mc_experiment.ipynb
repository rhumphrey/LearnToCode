{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24100339-c79a-4bc8-82f4-69fa5439131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "import time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc3613-aa0a-41e1-9eff-7b238dd96002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to test.  Add more as desired.\n",
    "models = [\n",
    "    \"llama3.1\",\n",
    "    \"mistral\",\n",
    "    \"gemma3\",\n",
    "    \"qwen2.5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67eedf-ec49-4cab-8a69-fe7be318f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt (comment/uncomment below to try different types of prompts)\n",
    "prompt = \"Tell me a short story about a lost cat finding its way home\"\n",
    "# prompt = \"Explain the concept of large language models (LLMs) like I'm five.\"\n",
    "# prompt = \"Suggest five albums to listen to after listening to 'Nightmare Logic' by the band Power Trip\"\n",
    "# prompt = \"Write a haiku about a rainy day.\"\n",
    "# prompt = \"Explain quantum physics in the style of a pirate.\"\n",
    "# prompt = \"Generate a Python function that calculates an interesting number sequence.\"\n",
    "# prompt = \"Describe the feeling of being relaxed in 50 words or less.\"\n",
    "# prompt = \"If you were a sentient cloud, what would your day be like?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feae548-fee7-408f-8f1f-e2ac5cc6b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ollama_prompt(model, prompt):\n",
    "    \"\"\"\n",
    "    Runs a prompt against a specified Ollama model and returns the response.\n",
    "    Handles errors and timing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = ollama.generate(model=model, prompt=prompt)\n",
    "        end_time = time.time()\n",
    "        response_text = response['response'].strip()  # Get the text and remove leading/trailing spaces\n",
    "        generation_time = end_time - start_time\n",
    "        return response_text, generation_time, None  # Return None for error\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error with model {model}: {e}\"\n",
    "        print(error_message)  # Print the error message\n",
    "        return None, None, error_message # Return the error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd668ef-1be2-4303-84c2-153dc6482eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_model_if_needed(model_name):\n",
    "    \"\"\"\n",
    "    Attempts to pull the model.  Ollama handles checking if it exists.\n",
    "    Returns True on success, an error message on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ollama.pull(model=model_name)\n",
    "        print(f\"Model '{model_name}' pulled successfully/already existed.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error pulling model '{model_name}': {e}\"\n",
    "        print(error_message)\n",
    "        return error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f94a9-335a-4fe9-8bec-05dfe9a98a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_response(response):\n",
    "    \"\"\"\n",
    "    Analyzes the response for length, and very basic complexity.  This is rudimentary.\n",
    "    More sophisticated analysis could be added (e.g., using a dedicated NLP library).\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        return 0, 0  \n",
    "\n",
    "    word_count = len(response.split())\n",
    "    sentence_count = response.count('.') + response.count('!') + response.count('?') # very unsophisticated :)\n",
    "    # A very rough proxy for complexity: average words per sentence.\n",
    "    complexity = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    return word_count, complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612aa0c3-22a7-4a95-b080-99b6bc430f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(prompt, models):\n",
    "    \"\"\"\n",
    "    Runs the prompt against each model, gathers the results, and prints a comparison.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Pull models before running the prompt\n",
    "    for model in models:\n",
    "        pull_result = pull_model_if_needed(model)\n",
    "        if pull_result is not True: # check if there was an error pulling the model\n",
    "            print(f\"Skipping model {model} due to error: {pull_result}\")\n",
    "            models.remove(model) # remove the model from the list.\n",
    "\n",
    "    # iterate through models for the prompt\n",
    "    print(f\"\\nRunning prompt: {prompt}\")\n",
    "    for model in models:\n",
    "        print(f\"\\nRunning prompt with model: {model}\")\n",
    "        response, generation_time, error = run_ollama_prompt(model, prompt) # Capture error\n",
    "        if response is not None:\n",
    "            word_count, complexity = analyze_response(response)\n",
    "            results[model] = { \n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"word_count\": word_count,\n",
    "                \"complexity\": complexity,\n",
    "                \"generation_time\": generation_time,\n",
    "                \"error\": error,\n",
    "            }\n",
    "        else:\n",
    "            results[model] = {\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": None,\n",
    "                \"word_count\": None,\n",
    "                \"complexity\": None,\n",
    "                \"generation_time\": generation_time,\n",
    "                \"error\": error,\n",
    "            }\n",
    "\n",
    "    print(\"\\nComparison of Model Outputs:\")\n",
    "    for model, data in results.items(): \n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(f\"\\n  Prompt: {data['prompt']}\") \n",
    "        if data[\"error\"]:\n",
    "            print(f\"    Error: {data['error']}\")\n",
    "        else:\n",
    "            print(f\"    Response: {data['response']}\")\n",
    "            print(f\"    Word Count: {data['word_count']}\")\n",
    "            print(f\"    Complexity (avg words per sentence): {data['complexity']:.2f}\")\n",
    "            print(f\"    Generation Time: {data['generation_time']:.2f} seconds\")\n",
    "\n",
    "    return results  # Return the results dictionary, which includes the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa16d3-e270-4fe1-945b-deb7c1f76676",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model pull, prompt and response comparison in progress...\\n\")\n",
    "results = compare_models(prompt, models) # Pass the single prompt\n",
    "\n",
    "# You can further analyze the results here, e.g., save to a JSON file, etc.\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")  # Generate timestamp\n",
    "filename_json = f\"model_comparison_results_{timestamp}.json\"  # Include timestamp in filename\n",
    "filename_md = f\"model_comparison_results_{timestamp}.md\"\n",
    "\n",
    "with open(filename_json, \"w\") as f_json:\n",
    "    json.dump(results, f_json, indent=4) # indent for pretty printing\n",
    "\n",
    "with open(filename_md, \"w\", encoding=\"utf-8\") as f_md:\n",
    "    f_md.write(\"# Model Comparison Results\\n\\n\")\n",
    "    for model, data in results.items(): # Iterate directly over the models and their data\n",
    "        f_md.write(f\"## Model: {model}\\n\\n\")\n",
    "        f_md.write(f\"### Prompt: {data['prompt']}\\n\\n\")\n",
    "        if data[\"error\"]:\n",
    "            f_md.write(f\"**Error**: {data['error']}\\n\\n\")\n",
    "        else:\n",
    "            f_md.write(f\"**Response**: {data['response']}\\n\\n\")\n",
    "            f_md.write(f\"**Word Count**: {data['word_count']}\\n\\n\")\n",
    "            f_md.write(f\"**Complexity (avg words per sentence)**: {data['complexity']:.2f}\\n\\n\")\n",
    "            f_md.write(f\"**Generation Time**: {data['generation_time']:.2f} seconds\\n\\n\")\n",
    "print(f\"\\nResults saved to {filename_json} and {filename_md}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
