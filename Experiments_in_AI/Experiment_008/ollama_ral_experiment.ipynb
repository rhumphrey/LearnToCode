{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309abb9-a807-4412-a58c-453d50d467b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09463452-9987-461b-91b3-28b1b3a26bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_model_if_needed(model_name):\n",
    "    \"\"\"Pulls the specified model from Ollama if it doesn't exist.\"\"\"\n",
    "    try:\n",
    "        ollama.pull(model=model_name)\n",
    "        print(f\"Model '{model_name}' pulled successfully/already existed.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error pulling model '{model_name}': {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debf402-f483-4070-bd09-2f76cf28bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(model_name, prompt):\n",
    "    \"\"\"Runs a prompt on a given model and returns the response.\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error running prompt on model '{model_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7c495-9011-42df-a9be-cc917d64662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = [\"mistral\", \"llama3.1\", \"granite3.3\"]  # Adjust based on your setup\n",
    "logic_puzzles = [\n",
    "    {\n",
    "        \"question\": \"If John is taller than Mary, and Mary is taller than Sue, who is the tallest?\",\n",
    "        \"expected_answer\": \"John is the tallest.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"There are 5 apples and you take away 3. How many apples do you have?\",\n",
    "        \"expected_answer\": \"You have 3 apples.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A farmer has 17 sheep, and all but nine die. How many sheep are left?\",\n",
    "        \"expected_answer\": \"9 sheep are left.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If all bloops are floops, and some floops are gloops, are all bloops gloops?\",\n",
    "        \"expected_answer\": \"No, not necessarily.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A train leaves New York at 8 am traveling at 60 mph. Another train leaves Chicago at 9 am traveling at 70 mph. Assuming they are on the same track heading towards each other and the distance between New York and Chicago is 800 miles, at what time will they meet?\",\n",
    "        \"expected_answer\": \"The two trains will meet at approximately 2:42 PM.\" \n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"--- Testing Reasoning and Logic in Ollama Models ---\")\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n--- Testing model: {model_name} ---\")\n",
    "    if not pull_model_if_needed(model_name):\n",
    "        continue\n",
    "\n",
    "    for puzzle in logic_puzzles:\n",
    "        question = puzzle[\"question\"]\n",
    "        expected_answer = puzzle[\"expected_answer\"]\n",
    "\n",
    "        # Test with standard prompt\n",
    "        standard_prompt = question\n",
    "        answer_standard = run_prompt(model_name, standard_prompt)\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(f\"Expected Answer: {expected_answer}\")\n",
    "        print(f\"\\nAnswer (Standard Prompt):\\n {answer_standard}\")\n",
    "\n",
    "        # Test with \"Let's think step-by-step\" prompt\n",
    "        step_by_step_prompt = f\"Let's think step-by-step. {question}\"\n",
    "        answer_step_by_step = run_prompt(model_name, step_by_step_prompt)\n",
    "        print(f\"\\nAnswer (Step-by-Step Prompt):\\n {answer_step_by_step}\")\n",
    "\n",
    "print(\"\\n--- Observation ---\")\n",
    "print(\"Observe which models successfully solve the logic puzzles.\")\n",
    "print(\"Note if the 'Let's think step-by-step' prompt helps the model by showing its reasoning process (or maybe not).\")\n",
    "print(\"Identify cases where the models fail and try to understand the nature of the failure (e.g., misunderstanding the logic, calculation errors).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555c5a4-5888-472a-9d6e-c89299cf20f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
