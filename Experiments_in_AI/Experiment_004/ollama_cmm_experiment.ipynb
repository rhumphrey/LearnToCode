{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de17cf-db93-48fb-8f2c-0f06bddc90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65107fd1-09e9-4372-9baa-3c849cf0258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_models():\n",
    "    \"\"\"Lists the available models in Ollama along with their file sizes.\"\"\"\n",
    "    try:\n",
    "        models_info = ollama.list()['models']\n",
    "        if not models_info:\n",
    "            print(\"No models found in Ollama.\")\n",
    "            return []\n",
    "\n",
    "        print(\"\\n--- Available Ollama Models ---\")\n",
    "        model_names = []\n",
    "        for model in models_info:\n",
    "            name = model['model']\n",
    "            file_size = model['size']\n",
    "            family = model['details']['family']\n",
    "            param_size = model['details']['parameter_size']\n",
    "            quant_level = model['details']['quantization_level']\n",
    "            human_readable_size = _format_bytes(file_size)\n",
    "            print(f\"Model: {name}, File size: {human_readable_size}, Family: {family}, Parameter size: {param_size}, Parameter size: {quant_level}\")\n",
    "            model_names.append(name)\n",
    "        return model_names\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f544b4-3cde-4e58-9ad8-ca5f67e43b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_bytes(size_bytes):\n",
    "    \"\"\"Formats bytes into a human-readable string (KB, MB, GB).\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0 bytes\"\n",
    "    i = 0\n",
    "    power = 1024\n",
    "    size_labels = [\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    while size_bytes >= power and i < len(size_labels) - 1:\n",
    "        size_bytes /= power\n",
    "        i += 1\n",
    "    return f\"{size_bytes:.2f} {size_labels[i]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81857033-e2dd-4432-8ae9-047b87b73af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38661d-9fb9-49f9-b1de-c3f3690b0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ollama_query(model_name, prompt):\n",
    "    \"\"\"Sends a prompt to the specified Ollama model and returns the response.\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying model '{model_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21827b06-e65e-45b6-8aa8-7baa2ed276ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"gemma3:1b\"  # Ensure this is available\n",
    "pirate_model_name = \"mypirate\"\n",
    "low_temp_model_name = \"mypiratelowtemp\"\n",
    "high_temp_model_name = \"mypiratehightemp\"\n",
    "modelfile_content = f\"\"\"\n",
    "FROM {base_model}\n",
    "SYSTEM \\\"\\\"\\\"You are a helpful assistant that always responds in the style of a pirate.\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "# Create Modelfiles\n",
    "with open(\"Modelfile_base\", \"w\") as f:\n",
    "    f.write(modelfile_content + \"PARAMETER temperature 0.8\")\n",
    "with open(\"Modelfile_low_temp\", \"w\") as f:\n",
    "    f.write(modelfile_content + \"PARAMETER temperature 0.2\")\n",
    "with open(\"Modelfile_high_temp\", \"w\") as f:\n",
    "    f.write(modelfile_content + \"PARAMETER temperature 1.2\")\n",
    "\n",
    "# Create the custom models (you only need to do this once in your terminal)\n",
    "print(f\"Remember to create the custom models in your terminal using:\")\n",
    "print(f\"ollama create {pirate_model_name} -f ./Modelfile_base\")\n",
    "print(f\"ollama create {low_temp_model_name} -f ./Modelfile_low_temp\")\n",
    "print(f\"ollama create {high_temp_model_name} -f ./Modelfile_high_temp\")\n",
    "input(\"Press Enter when the models are created...\")\n",
    "\n",
    "# Interaction prompts\n",
    "prompts = [\n",
    "    \"Tell me about your ship.\",\n",
    "    \"What is your favorite treasure?\",\n",
    "    \"How do you navigate the seas?\",\n",
    "    \"What do you think of other AI assistants?\",\n",
    "    \"Translate 'hello' into pirate speak.\",\n",
    "    \"Write a Python function to calculate the factorial of a number\"\n",
    "]\n",
    "\n",
    "print(f\"\\n--- Interacting with {pirate_model_name} (Temperature 0.8) ---\")\n",
    "for prompt in prompts:\n",
    "    response = run_ollama_query(pirate_model_name, prompt)\n",
    "    if response:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "\n",
    "print(f\"\\n--- Interacting with {low_temp_model_name} (Temperature 0.2) ---\")\n",
    "for prompt in prompts:\n",
    "    response = run_ollama_query(low_temp_model_name, prompt)\n",
    "    if response:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "\n",
    "print(f\"\\n--- Interacting with {high_temp_model_name} (Temperature 1.2) ---\")\n",
    "for prompt in prompts:\n",
    "    response = run_ollama_query(high_temp_model_name, prompt)\n",
    "    if response:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "\n",
    "print(\"\\n--- Review the consistency of the pirate persona and the creativity of the responses across different temperatures. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d1808-6934-467e-aad5-1f1e05acea19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
