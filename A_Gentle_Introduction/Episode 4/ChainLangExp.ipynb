{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ed302-aca6-47fc-85dc-bbaf84b1427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation 1 - LangChain-Ollama\n",
    "# Install the langchain-ollama package and ensure it's up-to-date.\n",
    "# The \"-q\" flag suppresses output, and \"U\" forces an upgrade to the latest version.\n",
    "%pip install -qU langchain-ollama\n",
    "\n",
    "# ChatOllama Reference - https://python.langchain.com/docs/integrations/chat/ollama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f13509-1ac4-4e82-a708-a2b3037dd20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation 2 - Ollama\n",
    "# Install the ollama package and upgrade it to the latest version if an older version is already installed.\n",
    "%pip install -U ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf34a91-9f4c-4760-96c7-2665127b5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "# Import the ChatOllama class from the langchain_ollama package.\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize a ChatOllama object to configure the language model.\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",   # Specify the model version to use (in this case, \"llama3.2\").\n",
    "    temperature=0,      # Set the temperature to 0 for deterministic output (no randomness in responses).\n",
    "    # other params...   # Placeholder for additional parameters you may want to configure.\n",
    ")\n",
    "\n",
    "# API Reference - https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0874e-ad7b-47c0-a0c7-e01dcd0c2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocation\n",
    "# Import the AIMessage class from the langchain_core.messages module.\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Create a list of messages to be passed to the language model.\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",  # Role of the message (\"system\" provides initial instructions to the model).\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",  \n",
    "        # Instruction to the assistant, setting its task as translating English to French.\n",
    "    ),\n",
    "    (\n",
    "        \"human\",  # Role of the message (\"human\" represents the user's input).\n",
    "        \"I love programming.\",  # The user's sentence to be translated into French.\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Use the invoke method of the llm object (ChatOllama) to process the list of messages.\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "# Output the response from the language model (translation result).\n",
    "ai_msg\n",
    "\n",
    "# API Reference - https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac0df5-465c-41c7-9c2b-83655b5e7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the content of the AIMessage object (the translated sentence from the language model's response).\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625e482-e266-49dc-9eff-b63288e921f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected output for validation.\n",
    "expected_output = \"Je aime le programmation.\"  # Replace with the correct translation.\n",
    "\n",
    "# Compare the AI's response to the expected output. You might want a more nuanced check here... (case-insensitive/partial/external/fuzzy matching)\n",
    "if ai_msg.content.strip() == expected_output:\n",
    "    print(\"Output is correct!\")\n",
    "else:\n",
    "    print(\"Output is incorrect. Got:\", ai_msg_chain.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939c2e0-34cf-45ae-b531-9d5dc85e134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaining\n",
    "# Import the ChatPromptTemplate class from the langchain_core.prompts module.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a chat prompt template that generates messages dynamically.\n",
    "# The \"system\" message defines the assistant's role and task, using placeholders for input and output languages.\n",
    "# The \"human\" message is a placeholder for the user's input, making the template flexible for various use cases.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",  \n",
    "        ),\n",
    "        (\"human\", \"{input}\"),  # Placeholder for the user's input text to be translated.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine (pipe) the prompt with the language model (llm) to form a processing chain.\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chain with specific inputs to perform the translation task.\n",
    "ai_msg_chain = chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",  # Specify the input language (English in this case).\n",
    "        \"output_language\": \"German\", # Specify the output language (German in this case).\n",
    "        \"input\": \"I love programming.\",  # Provide the text to be translated.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output the result of the chain's invocation.\n",
    "ai_msg_chain\n",
    "\n",
    "# API Reference - https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c46bf6-77c2-478f-9978-7141b5af7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the content of the AIMessage object (the translated text from the language model's response).\n",
    "print(ai_msg_chain.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631832c-23d4-4fd8-93a6-9f6dbc3b6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected output for validation.\n",
    "expected_output = \"Ich liebe Programmierung.\"  # Replace with the correct translation.\n",
    "\n",
    "# Compare the AI's response to the expected output. You might want a more nuanced check here...\n",
    "if ai_msg_chain.content.strip() == expected_output:\n",
    "    print(\"Output is correct!\")\n",
    "else:\n",
    "    print(\"Output is incorrect. Got:\", ai_msg_chain.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad54d9-471c-4534-968c-9b2486e4794a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
