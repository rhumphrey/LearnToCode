{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20798de3-60f7-4c25-b466-24b1ce2ebf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0567f1-6349-46d4-b8e7-4319b004f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://localhost:11434\"\n",
    "model    = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a28287-c219-472a-8ef3-528a404b80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model_name, prompt, options=None, stream=False, endpoint_url=None):\n",
    "    \"\"\"\n",
    "    Generates a response for a given prompt with a provided model using the /api/generate endpoint.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the Ollama model to use (e.g., \"llama3.2\", \"mistral\").\n",
    "        prompt (str): The text prompt to send to the model.\n",
    "        options (dict, optional):  Options to control generation, like temperature, top_p, etc. Defaults to None.\n",
    "                                    See Ollama API documentation for available options.\n",
    "        stream (bool, optional): Whether to stream the response. Defaults to False.\n",
    "        endpoint_url (str, optional):  Full endpoint URL if you want to override the default. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The response object from the API call.\n",
    "                           You can access the JSON response using response.json().\n",
    "                           Returns None if there's an exception during the API call.\n",
    "    \"\"\"\n",
    "    url = endpoint_url if endpoint_url else f\"{base_url}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": stream,\n",
    "    }\n",
    "    if options:\n",
    "        payload[\"options\"] = options\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error in generate_response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172361a-f4b5-4637-ae74-99527532abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Tell me a joke.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e5aa5-4e59-4e10-8575-53d78e9d8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generate = generate_response(model_name=model, prompt=user_prompt)\n",
    "if response_generate:\n",
    "    print(\"Generate Response:\")\n",
    "    print(response_generate.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8750169-46f0-4331-b90b-a9e1a6ba90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_generate.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06656345-6d4d-47a3-bdea-58b17cbafc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What is a good first programming language to learn?\"\n",
    "predictable = {\n",
    "    \"temperature\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31d866-4586-4af7-b002-9fc2888bdb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generate = generate_response(model_name=model, prompt=user_prompt, options=predictable)\n",
    "if response_generate:\n",
    "    print(\"Generate Response:\")\n",
    "    print(response_generate.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93778ed-cc56-4665-82d2-e165df4b15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_generate.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82dadc8-2482-493e-986a-d0ca07263e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Tell me a story about the time that Mav the Cat tried to learn quantum programming and inadvertently altered the past\"\n",
    "creative = {\n",
    "    \"temperature\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dff60b-b219-4dd1-88a9-f075a6939004",
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_response_generate = generate_response(model_name=model, prompt=user_prompt, options=creative)\n",
    "if creative_response_generate:\n",
    "    print(\"Generate Response:\")\n",
    "    print(creative_response_generate.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f658aa-b158-4abe-ace3-14b57ac7ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(creative_response_generate.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd5238-50c2-45b0-8f3a-ec419f6358cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(model_name, messages, options=None, stream=False, endpoint_url=None):\n",
    "    \"\"\"\n",
    "    Generates the next message in a chat with a provided model using the /api/chat endpoint.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the Ollama model to use (e.g., \"llama3.2\", \"mistral\").\n",
    "        messages (list): A list of message dictionaries representing the chat history.\n",
    "                         Each message should be a dictionary with \"role\" and \"content\" keys.\n",
    "                         e.g., [{\"role\": \"user\", \"content\": \"Hello\"}, {\"role\": \"assistant\", \"content\": \"Hi there!\"}]\n",
    "        options (dict, optional): Options to control generation, like temperature, top_p, etc. Defaults to None.\n",
    "                                  See Ollama API documentation for available options.\n",
    "        stream (bool, optional): Whether to stream the response. Defaults to False.\n",
    "        endpoint_url (str, optional): Full endpoint URL if you want to override the default. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The response object from the API call.\n",
    "                           You can access the JSON response using response.json().\n",
    "                           Returns None if there's an exception during the API call.\n",
    "    \"\"\"\n",
    "    url = endpoint_url if endpoint_url else f\"{base_url}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": stream,\n",
    "    }\n",
    "    if options:\n",
    "        payload[\"options\"] = options\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error in chat_completion: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a8138-4b09-4f9c-8c83-178c07b99fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Completion 1 - Chat\n",
    "messages_history = [{\"role\": \"user\", \"content\": \"Hello\"}, \n",
    "                    {\"role\": \"assistant\", \"content\": \"Hi there!\"}, \n",
    "                    {\"role\": \"user\", \"content\": \"How are you?\"}]\n",
    "response_chat = chat_completion(model_name=model, messages=messages_history)\n",
    "if response_chat:\n",
    "    print(\"\\nChat Completion Response:\")\n",
    "    print(response_chat.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb7acb-34c1-46d9-b6b4-f89fb33cd913",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_chat.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954d56f-d282-4a72-a94d-16ccb4ea6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Completion 2 - Persona\n",
    "messages_persona = [{\"role\": \"system\", \"content\": \"You are a tuxedo cat called Mav\"}, \n",
    "                    {\"role\": \"user\", \"content\": \"What is your name?\"},]\n",
    "response_chat = chat_completion(model_name=model, messages=messages_persona)\n",
    "if response_chat:\n",
    "    print(\"\\nChat Completion Response:\")\n",
    "    print(response_chat.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec04a72-c1ce-42f7-8b4c-e049fa86aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_chat.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5675c4-cd01-46a8-8c96-2589be5aa9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_info(model_name, endpoint_url=None):\n",
    "    \"\"\"\n",
    "    Shows information about a model including details, Modelfile, template, parameters, license, system prompt using the /api/show endpoint.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the Ollama model to get information about (e.g., \"llama3.2\", \"mistral\").\n",
    "        endpoint_url (str, optional): Full endpoint URL if you want to override the default. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The response object from the API call.\n",
    "                           You can access the JSON response using response.json().\n",
    "                           Returns None if there's an exception during the API call.\n",
    "    \"\"\"\n",
    "    url = endpoint_url if endpoint_url else f\"{base_url}/api/show\"\n",
    "    payload = {\n",
    "        \"name\": model_name,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error in show_model_info: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d058b00-4487-4b12-9c13-50ba39c16463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Model Info\n",
    "response_show = show_model_info(model_name=model)\n",
    "if response_show:\n",
    "    print(\"\\nShow Model Info:\")\n",
    "    print(response_show.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773c4d7-7f68-4024-86bb-f5ceab506f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_show.json()['details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734e723-3c7d-4079-8276-53d9039a100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_show.json()['details']['family'])\n",
    "print(response_show.json()['details']['parameter_size'])\n",
    "print(response_show.json()['details']['quantization_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce9e9b-dea9-4af8-bde5-804dee0898c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_models(endpoint_url=None):\n",
    "    \"\"\"\n",
    "    Lists models that are available locally using the /api/tags endpoint.\n",
    "\n",
    "    Args:\n",
    "        endpoint_url (str, optional): Full endpoint URL if you want to override the default. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The response object from the API call.\n",
    "                           You can access the JSON response using response.json().\n",
    "                           Returns None if there's an exception during the API call.\n",
    "    \"\"\"\n",
    "    url = endpoint_url if endpoint_url else f\"{base_url}/api/tags\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error in list_available_models: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c0761-82e0-4668-ad0d-4e8d2ebe5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Available Models\n",
    "response_tags = list_available_models()\n",
    "if response_tags:\n",
    "    print(\"\\nAvailable Models:\")\n",
    "    print(response_tags.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dac42d-6f51-4d8a-834e-2e776339e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = response_tags.json()['models']\n",
    "model_names = [model['name'] for model in models_list]\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a14c76-4fca-460e-a4fd-ee629bbae3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using a for loop to print the names on seperate lines\n",
    "for model_name in model_names:\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc4a6e0-922b-4dac-9010-4af861a16110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using the * operator with print (less common for this exact purpose, but demonstrates unpacking)\n",
    "print(*model_names, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3bf95-2e7e-497e-b7bb-33028a7a95d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_loaded_models(endpoint_url=None):\n",
    "    \"\"\"\n",
    "    Lists models that are currently loaded into memory using the /api/ps endpoint.\n",
    "\n",
    "    Args:\n",
    "        endpoint_url (str, optional): Full endpoint URL if you want to override the default. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The response object from the API call.\n",
    "                           You can access the JSON response using response.json().\n",
    "                           Returns None if there's an exception during the API call.\n",
    "    \"\"\"\n",
    "    url = endpoint_url if endpoint_url else f\"{base_url}/api/ps\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error in list_loaded_models: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c675a-880c-4937-a706-b1ef377d3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Loaded Models\n",
    "response_ps = list_loaded_models()\n",
    "if response_ps:\n",
    "    print(\"\\nLoaded Models:\")\n",
    "    print(response_ps.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade7fd1-e963-46d3-b7a7-f6dd5d168577",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models_list = response_ps.json()['models']\n",
    "loaded_model_names = [model['name'] for model in loaded_models_list]\n",
    "print(loaded_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618fd147-b690-4f29-91fd-55a3c2de02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*loaded_model_names, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefd23d-9d21-4d2e-9280-626947092a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_version(endpoint_url=None):\n",
    "    \"\"\"\n",
    "    Retrieves the Ollama version using the /api/version endpoint.\n",
    "\n",
    "    Args:\n",
    "        endpoint_url (str, optional): Full endpoint URL if you want to override the default. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The response object from the API call.\n",
    "                           You can access the JSON response using response.json().\n",
    "                           Returns None if there's an exception during the API call.\n",
    "    \"\"\"\n",
    "    url = endpoint_url if endpoint_url else f\"{base_url}/api/version\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error in get_ollama_version: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54109a-15e5-4c56-9059-32b0e908a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Ollama Version\n",
    "response_version = get_ollama_version()\n",
    "if response_version:\n",
    "    print(\"\\nOllama Version:\")\n",
    "    print(response_version.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893043b-40ca-4aef-9971-b67dc7184212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_version.json()['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc45e3-0811-4b57-bfb4-be50bf23f5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
