{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7732d928-7e0f-43ce-af67-b6d49b9fe3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama  # Make sure you've `pip install ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd95a55-5e38-43d2-9b33-3d8ac28cefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Specialized Task Delegation (Summarizer + Elaborator)\n",
    "\n",
    "def summarize_and_elaborate(text):\n",
    "    print(\"--- Experiment 1: Summarizer + Elaborator ---\")\n",
    "\n",
    "    # LLM 1: Summarizer (using Phi-3 for speed)\n",
    "    print(\"\\n[Phi-3] Summarizing text...\")\n",
    "    summary_response = ollama.chat(\n",
    "        model='phi3',\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': f'Summarize the following text concisely:\\n\\n{text}'},\n",
    "        ]\n",
    "    )\n",
    "    summary = summary_response['message']['content']\n",
    "    print(f\"Summary: {summary}\")\n",
    "\n",
    "    # LLM 2: Elaborator (using Llama 3 for detail)\n",
    "    print(\"\\n[Llama 3] Elaborating on the summary...\")\n",
    "    elaboration_response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': f'The following is a summary: \"{summary}\". Expand on one key point from this summary, providing more detail or context.'},\n",
    "        ]\n",
    "    )\n",
    "    elaboration = elaboration_response['message']['content']\n",
    "    print(f\"Elaboration: {elaboration}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test\n",
    "long_text = \"\"\"\n",
    "Foundation models are large-scale machine learning models trained on vast amounts of broad, unlabeled data (e.g., text, images, code). They are designed to be adaptable to a wide range of downstream tasks with minimal fine-tuning, often through techniques like few-shot or zero-shot prompting. Large Language Models (LLMs) like GPT-4 are the most prominent examples, but foundation models also exist for vision, code, audio, and increasingly, multiple modalities.\n",
    "\"\"\"\n",
    "summarize_and_elaborate(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9e493-230f-40a3-87dd-a284dc55f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Simple RAG (Retrieval Augmented Generation)\n",
    "\n",
    "import chromadb # Make sure you've `pip install chromadb`\n",
    "\n",
    "def simple_rag_experiment(query):\n",
    "    print(\"--- Experiment 2: Simple RAG ---\")\n",
    "\n",
    "    # 1. Define documents (our \"knowledge base\")\n",
    "    documents = [\n",
    "        \"The capital of France is Paris.\",\n",
    "        \"The Eiffel Tower is located in Paris.\",\n",
    "        \"Berlin is the capital of Germany.\",\n",
    "        \"Mount Everest is the highest mountain in the world.\"\n",
    "    ]\n",
    "\n",
    "    # 2. Create a ChromaDB client and collection\n",
    "    client = chromadb.Client()\n",
    "    collection_name = \"my_ollama_rag_collection\"\n",
    "    try:\n",
    "        # Try to get existing collection, or create a new one\n",
    "        collection = client.get_or_create_collection(name=collection_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not get or create collection directly. Trying to delete and recreate. Error: {e}\")\n",
    "        client.delete_collection(name=collection_name)\n",
    "        collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    # Add documents to the collection if it's empty\n",
    "    if collection.count() == 0:\n",
    "        print(\"[ChromaDB] Adding documents to collection (using nomic-embed-text for embeddings)...\")\n",
    "        # Ollama's `embeddings` API\n",
    "        embeddings = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            embed_response = ollama.embeddings(model='nomic-embed-text', prompt=doc)\n",
    "            embeddings.append(embed_response['embedding'])\n",
    "        collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=documents,\n",
    "            ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    "        )\n",
    "        print(f\"[ChromaDB] Added {len(documents)} documents.\")\n",
    "    else:\n",
    "        print(f\"[ChromaDB] Collection '{collection_name}' already contains {collection.count()} documents.\")\n",
    "\n",
    "\n",
    "    # 3. Embed the query\n",
    "    print(f\"\\n[nomic-embed-text] Embedding query: '{query}'\")\n",
    "    query_embed_response = ollama.embeddings(model='nomic-embed-text', prompt=query)\n",
    "    query_embedding = query_embed_response['embedding']\n",
    "\n",
    "    # 4. Search for relevant documents\n",
    "    print(\"[ChromaDB] Searching for relevant documents...\")\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=1\n",
    "    )\n",
    "    relevant_doc = results['documents'][0][0] if results['documents'] and results['documents'][0] else \"No relevant document found.\"\n",
    "    print(f\"Relevant document found: '{relevant_doc}'\")\n",
    "\n",
    "    # 5. Generate response using Llama 3 with context\n",
    "    print(\"\\n[Llama 3] Generating answer with context...\")\n",
    "    context = f\"Context: {relevant_doc}\\nQuestion: {query}\"\n",
    "    response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant. Use the provided context to answer the question. If the answer is not in the context, state that you don\\'t know.'},\n",
    "            {'role': 'user', 'content': context}\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Answer: {response['message']['content']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test\n",
    "simple_rag_experiment(\"What is the capital of France?\")\n",
    "simple_rag_experiment(\"Where is the Eiffel Tower located?\")\n",
    "simple_rag_experiment(\"What is the highest mountain?\")\n",
    "simple_rag_experiment(\"Who painted the Mona Lisa?\") # Should say it doesn't know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e4378-3731-4a88-916a-e407f799d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Creative Collaboration (Idea Generator + Elaborator)\n",
    "\n",
    "def creative_collaboration(theme):\n",
    "    print(\"--- Experiment 3: Creative Collaboration ---\")\n",
    "\n",
    "    # LLM 1: Idea Generator (Phi-3 for quick ideas)\n",
    "    print(f\"\\n[Phi-3] Generating a story idea about: {theme}\")\n",
    "    idea_prompt = f\"Generate a very brief, interesting concept for a short story about '{theme}'. Just the core idea.\"\n",
    "    idea_response = ollama.chat(\n",
    "        model='phi3',\n",
    "        messages=[{'role': 'user', 'content': idea_prompt}]\n",
    "    )\n",
    "    story_idea = idea_response['message']['content']\n",
    "    print(f\"Story Idea: {story_idea}\")\n",
    "\n",
    "    # LLM 2: Elaborator/Storyteller (Llama 3 for more narrative)\n",
    "    print(\"\\n[Llama 3] Expanding on the idea into a short narrative...\")\n",
    "    story_prompt = f\"Based on the following core idea, write a very short, engaging story (2-3 sentences):\\n\\nCore Idea: {story_idea}\"\n",
    "    story_response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[{'role': 'user', 'content': story_prompt}]\n",
    "    )\n",
    "    short_story = story_response['message']['content']\n",
    "    print(f\"Short Story: {short_story}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test\n",
    "creative_collaboration(\"a lost ancient artifact\")\n",
    "creative_collaboration(\"a futuristic city where dreams are shared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403bbf3-7041-40da-823e-6f6ba781292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Self-Correction Loop (Simple Refinement)\n",
    "\n",
    "def self_correction_experiment(initial_prompt):\n",
    "    print(\"--- Experiment 4: Self-Correction Loop ---\")\n",
    "\n",
    "    # Initial Generation (Llama 3)\n",
    "    print(f\"\\n[Llama 3] Initial generation for: '{initial_prompt}'\")\n",
    "    initial_response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[{'role': 'user', 'content': initial_prompt}]\n",
    "    )\n",
    "    generated_text = initial_response['message']['content']\n",
    "    print(f\"Initial Text:\\n{generated_text}\")\n",
    "\n",
    "    # Critique (Llama 3 critiquing itself)\n",
    "    print(\"\\n[Llama 3] Critiquing the generated text...\")\n",
    "    critique_prompt = f\"\"\"\n",
    "    Here is a piece of text:\n",
    "    \"{generated_text}\"\n",
    "\n",
    "    Critique this text. Identify one specific area for improvement (e.g., make it more concise, add more detail, improve clarity, fix grammar, be more engaging). Provide a single, actionable suggestion for improvement.\n",
    "    \"\"\"\n",
    "    critique_response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[{'role': 'user', 'content': critique_prompt}]\n",
    "    )\n",
    "    critique = critique_response['message']['content']\n",
    "    print(f\"Critique: {critique}\")\n",
    "\n",
    "    # Refinement (Llama 3 refining based on critique)\n",
    "    print(\"\\n[Llama 3] Refining text based on critique...\")\n",
    "    refine_prompt = f\"\"\"\n",
    "    Here is a piece of text:\n",
    "    \"{generated_text}\"\n",
    "\n",
    "    Here is a critique and suggestion for improvement:\n",
    "    \"{critique}\"\n",
    "\n",
    "    Rewrite the original text incorporating this specific suggestion.\n",
    "    \"\"\"\n",
    "    refined_response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[{'role': 'user', 'content': refine_prompt}]\n",
    "    )\n",
    "    refined_text = refined_response['message']['content']\n",
    "    print(f\"Refined Text:\\n{refined_text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test\n",
    "self_correction_experiment(\"Write a short paragraph about the benefits of waking up early.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d1bb7-c014-4924-9d61-e18cbdc4989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: Role-Based Q&A (Domain Expert Routing)\n",
    "\n",
    "def role_based_qa(question):\n",
    "    print(\"--- Experiment 5: Role-Based Q&A ---\")\n",
    "\n",
    "    # Simple keyword-based routing (can be more complex with another LLM or NER)\n",
    "    if \"code\" in question.lower() or \"programming\" in question.lower() or \"function\" in question.lower():\n",
    "        model_to_use = 'llama3' # Llama 3 often handles code reasonably well\n",
    "        print(f\"\\n[Routing] Question seems code-related, routing to {model_to_use}...\")\n",
    "        prompt = f\"As a programming expert, answer the following: {question}\"\n",
    "    elif \"story\" in question.lower() or \"creative\" in question.lower():\n",
    "        model_to_use = 'llama3' # Llama 3 for creative tasks\n",
    "        print(f\"\\n[Routing] Question seems creative, routing to {model_to_use}...\")\n",
    "        prompt = f\"As a creative writer, respond to the following: {question}\"\n",
    "    else:\n",
    "        model_to_use = 'phi3' # Default or general purpose model\n",
    "        print(f\"\\n[Routing] General question, routing to {model_to_use}...\")\n",
    "        prompt = question\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model_to_use,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    print(f\"Answer from {model_to_use}:\\n{response['message']['content']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test\n",
    "role_based_qa(\"Write a very short Python function for a Fibonacci sequence.\")\n",
    "role_based_qa(\"Tell me a very short, imaginative story about a talking cat.\")\n",
    "role_based_qa(\"What is the capital of Canada?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4808625-6580-4a1e-8cd7-bda68770396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6: A/B Testing Responses\n",
    "\n",
    "def ab_test_responses(prompt, model_a='llama3', model_b='phi3'):\n",
    "    print(\"--- Experiment 6: A/B Testing Responses ---\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "\n",
    "    # Get response from Model A\n",
    "    print(f\"\\n[Model A: {model_a}] Generating response...\")\n",
    "    response_a = ollama.chat(\n",
    "        model=model_a,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    output_a = response_a['message']['content']\n",
    "    print(f\"Response from {model_a}:\\n{output_a}\")\n",
    "\n",
    "    # Get response from Model B\n",
    "    print(f\"\\n[Model B: {model_b}] Generating response...\")\n",
    "    response_b = ollama.chat(\n",
    "        model=model_b,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    output_b = response_b['message']['content']\n",
    "    print(f\"Response from {model_b}:\\n{output_b}\")\n",
    "\n",
    "    print(\"\\n--- Comparison ---\")\n",
    "    print(f\"Length Model A ({len(output_a.split())} words): {output_a[:100]}...\") # Show first 100 chars\n",
    "    print(f\"Length Model B ({len(output_b.split())} words): {output_b[:100]}...\")\n",
    "    # You could add more sophisticated comparison here (e.g., keyword presence, sentiment, another LLM's critique)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test\n",
    "ab_test_responses(\"Describe a futuristic city with flying cars.\", model_a='llama3', model_b='phi3')\n",
    "ab_test_responses(\"Explain the concept of quantum entanglement in simple terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45c8b9-8cba-418c-bcfd-3ca1e253192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7: Content Moderation/Safety Filter\n",
    "\n",
    "def content_moderation_experiment(user_input, model_to_generate='llama3', model_to_moderate='phi3'):\n",
    "    print(\"--- Experiment 7: Content Moderation/Safety Filter ---\")\n",
    "\n",
    "    # LLM 1: Content Generation\n",
    "    print(f\"\\n[{model_to_generate}] Generating content based on: '{user_input}'\")\n",
    "    generated_content_response = ollama.chat(\n",
    "        model=model_to_generate,\n",
    "        messages=[{'role': 'user', 'content': user_input}]\n",
    "    )\n",
    "    generated_content = generated_content_response['message']['content']\n",
    "    print(f\"Generated Content:\\n{generated_content}\")\n",
    "\n",
    "    # LLM 2: Moderation/Safety Check\n",
    "    print(f\"\\n[{model_to_moderate}] Moderating the generated content...\")\n",
    "    moderation_prompt = f\"\"\"\n",
    "    Review the following text for any inappropriate, offensive, or harmful content.\n",
    "    If it is safe, respond with \"SAFE\".\n",
    "    If it contains inappropriate content, respond with \"FLAGGED\" and briefly explain why, suggesting a safer alternative if possible.\n",
    "\n",
    "    Text to review:\n",
    "    \"{generated_content}\"\n",
    "    \"\"\"\n",
    "    moderation_response = ollama.chat(\n",
    "        model=model_to_moderate,\n",
    "        messages=[{'role': 'user', 'content': moderation_prompt}]\n",
    "    )\n",
    "    moderation_result = moderation_response['message']['content'].strip()\n",
    "    print(f\"Moderation Result: {moderation_result}\")\n",
    "\n",
    "    if \"FLAGGED\" in moderation_result.upper():\n",
    "        print(\"\\nACTION REQUIRED: Content was flagged!\")\n",
    "    else:\n",
    "        print(\"\\nContent deemed safe.\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test with potentially benign input\n",
    "content_moderation_experiment(\"Write a happy short poem about a cat.\")\n",
    "# Test with input that might generate something questionable (be careful with actual explicit content)\n",
    "# This example is still mild but aims to show the moderation attempting to work.\n",
    "content_moderation_experiment(\"Write a frustrated rant about bad drivers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752952c8-d2bb-411c-bfc6-4748176ceb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 8: Persona-Based Dialogue\n",
    "\n",
    "def persona_based_dialogue(topic, user_query):\n",
    "    print(\"--- Experiment 8: Persona-Based Dialogue ---\")\n",
    "\n",
    "    # LLM 1: Persona Generator (e.g., Llama 3 to create a detailed persona)\n",
    "    print(f\"\\n[Llama 3] Generating a persona for a discussion about '{topic}'...\")\n",
    "    persona_prompt = f\"Create a detailed and interesting persona for a helpful assistant who is an expert in '{topic}'. Describe their background, typical tone, and what kind of advice they'd give.\"\n",
    "    persona_response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[{'role': 'user', 'content': persona_prompt}]\n",
    "    )\n",
    "    persona_description = persona_response['message']['content']\n",
    "    print(f\"Generated Persona:\\n{persona_description}\")\n",
    "\n",
    "    # LLM 2: Dialogue Agent (e.g., Phi-3 or Llama 3 acting as the persona)\n",
    "    print(f\"\\n[Phi-3] Responding as the generated persona to query: '{user_query}'...\")\n",
    "    dialogue_prompt = f\"\"\"\n",
    "    You are to act as the following persona:\n",
    "    {persona_description}\n",
    "\n",
    "    Now, respond to the following user query:\n",
    "    \"{user_query}\"\n",
    "    \"\"\"\n",
    "    dialogue_response = ollama.chat(\n",
    "        model='phi3', # Or 'llama3' for more complex personas/responses\n",
    "        messages=[{'role': 'user', 'content': dialogue_prompt}]\n",
    "    )\n",
    "    persona_response_text = dialogue_response['message']['content']\n",
    "    print(f\"Persona's Response:\\n{persona_response_text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test\n",
    "persona_based_dialogue(\"gardening\", \"What's the best way to deal with aphids on roses?\")\n",
    "persona_based_dialogue(\"space travel\", \"What are the biggest challenges of long-duration space missions?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b1084-35d1-4cb6-b959-62de65be1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 9: Query Expansion / Reframing\n",
    "\n",
    "def query_expansion_experiment(original_query):\n",
    "    print(\"--- Experiment 9: Query Expansion / Reframing ---\")\n",
    "\n",
    "    # LLM 1: Query Expander/Reframer (e.g., Phi-3 for quick processing)\n",
    "    print(f\"\\n[Phi-3] Expanding/reframing the query: '{original_query}'\")\n",
    "    expansion_prompt = f\"\"\"\n",
    "    The user has asked: \"{original_query}\"\n",
    "    This query is a bit vague. Rephrase or expand this query into a more detailed and effective prompt that would get a better, more comprehensive answer from a sophisticated AI. Focus on clarifying intent or adding relevant context.\n",
    "    The output should be just the refined prompt.\n",
    "    \"\"\"\n",
    "    expanded_query_response = ollama.chat(\n",
    "        model='phi3',\n",
    "        messages=[{'role': 'user', 'content': expansion_prompt}]\n",
    "    )\n",
    "    expanded_query = expanded_query_response['message']['content'].strip()\n",
    "    print(f\"Expanded Query: {expanded_query}\")\n",
    "\n",
    "    # LLM 2: Answer Generator (e.g., Llama 3 for detailed responses)\n",
    "    print(f\"\\n[Llama 3] Answering based on the expanded query...\")\n",
    "    answer_response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[{'role': 'user', 'content': expanded_query}]\n",
    "    )\n",
    "    final_answer = answer_response['message']['content']\n",
    "    print(f\"Final Answer:\\n{final_answer}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test\n",
    "query_expansion_experiment(\"tell me about AI\")\n",
    "query_expansion_experiment(\"benefits of exercise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b9c14-225c-428e-b9ea-f082b51d2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 10: Asynchronous A/B Testing with Quality Scoring\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def generate_and_score(model_name, prompt, scoring_model='llama3'):\n",
    "    \"\"\"Generates a response and then gets a score from a separate LLM.\"\"\"\n",
    "    print(f\"  - [{model_name}] Generating response...\")\n",
    "    generation_response = await asyncio.to_thread(\n",
    "        ollama.chat,\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    generated_text = generation_response['message']['content']\n",
    "\n",
    "    print(f\"  - [{scoring_model}] Scoring response from {model_name}...\")\n",
    "    scoring_prompt = f\"\"\"\n",
    "    Rate the following response to the prompt: \"{prompt}\" on a scale of 1 to 10 (1 being very poor, 10 being excellent) based on its helpfulness, accuracy, and completeness.\n",
    "    Provide only the numerical score.\n",
    "\n",
    "    Response to rate:\n",
    "    \"{generated_text}\"\n",
    "    \"\"\"\n",
    "    scoring_response = await asyncio.to_thread(\n",
    "        ollama.chat,\n",
    "        model=scoring_model,\n",
    "        messages=[{'role': 'user', 'content': scoring_prompt}]\n",
    "    )\n",
    "    score_text = scoring_response['message']['content'].strip()\n",
    "    try:\n",
    "        score = int(score_text.splitlines()[0].strip()) # Try to extract just the number\n",
    "    except ValueError:\n",
    "        score = 0 # Default if scoring model doesn't return a clear number\n",
    "\n",
    "    return model_name, generated_text, score\n",
    "\n",
    "async def async_ab_testing_with_scoring(prompt, models_to_test=['llama3', 'phi3']):\n",
    "    print(\"--- Experiment 10: Asynchronous A/B Testing with Quality Scoring ---\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "\n",
    "    tasks = [generate_and_score(model, prompt) for model in models_to_test]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"\\n--- Results Summary ---\")\n",
    "    best_model = None\n",
    "    best_score = -1\n",
    "    for model_name, generated_text, score in results:\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        print(f\"  Score: {score}/10\")\n",
    "        print(f\"  Response (first 100 chars): {generated_text[:100]}...\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model_name\n",
    "\n",
    "    print(f\"\\nConclusion: {best_model} performed best with a score of {best_score}/10.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "async def main():\n",
    "    await async_ab_testing_with_scoring(\"Explain the concept of recursion in programming in a simple way.\")\n",
    "    await async_ab_testing_with_scoring(\"Write a short, uplifting haiku about nature.\")\n",
    "\n",
    "# --- NOTICE HOW WE CALL MAIN IN A JUPYTER NOTEBOOK WHEN USING asyncio.run() ---\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
