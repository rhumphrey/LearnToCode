{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc11536-5b1f-400f-af79-a702512f14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "# Assuming llama2 is available in your local ollama\n",
    "MODEL_NAME = \"llama2\"\n",
    "print(f\"\\n--- Prompt Augmentation, Engineering & Optimization (using {MODEL_NAME}) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29f58d-2024-4582-b139-aa0c14484358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Basic Prompting\n",
    "print(\"\\nExp 1: Basic Prompting\")\n",
    "response = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': 'What is the capital of France?'}\n",
    "])\n",
    "print(f\"Response: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c379d92-9095-44b4-902d-26963bb5950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Zero-Shot Summarization\n",
    "print(\"\\nExp 2: Zero-Shot Summarization\")\n",
    "text_to_summarize = \"The quick brown fox jumps over the lazy dog. This is a common pangram used to display typefaces. It contains every letter of the English alphabet.\"\n",
    "response = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': f'Summarize the following text: {text_to_summarize}'}\n",
    "])\n",
    "print(f\"Summary: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa01bf-564e-41ad-9e90-5023f6aac70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Few-Shot Summarization\n",
    "print(\"\\nExp 3: Few-Shot Summarization\")\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'Text: The sun is bright today. Summary: Bright sun.'},\n",
    "    {'role': 'assistant', 'content': 'Text: The dog barked loudly at the mailman. Summary: Loud dog bark.'},\n",
    "    {'role': 'user', 'content': f'Text: {text_to_summarize} Summary:'}\n",
    "]\n",
    "response = ollama.chat(model=MODEL_NAME, messages=messages)\n",
    "print(f\"Few-shot Summary: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6b3a0-6ed5-45a3-9942-64e9cec9a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Chain-of-Thought (CoT)\n",
    "print(\"\\nExp 4: Chain-of-Thought (CoT)\")\n",
    "cot_question = \"If a bus leaves New York at 8 AM and travels at 60 mph, and another bus leaves Boston at 9 AM and travels at 50 mph, when will they meet if the distance between them is 200 miles? Let's think step by step.\"\n",
    "response = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': cot_question}\n",
    "])\n",
    "print(f\"CoT Response: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bd537c-f5d9-4241-b226-d73863cc7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: Role-Playing\n",
    "print(\"\\nExp 5: Role-Playing\")\n",
    "response = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'system', 'content': 'You are a helpful Python programmer. Provide only code examples and explanations.'},\n",
    "    {'role': 'user', 'content': 'How do I read a file line by line in Python?'}\n",
    "])\n",
    "print(f\"Programmer Response: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3320b-6cea-474d-9d5c-9fe88061cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6: Constraint-Based Prompting\n",
    "print(\"\\nExp 6: Constraint-Based Prompting\")\n",
    "response = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': 'Generate a 50-word poem about nature. It must rhyme.'}\n",
    "])\n",
    "print(f\"Constraint-based Poem: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b19a5f-0bd8-465d-bba3-86cb78117d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7: Negative Prompting (simulated via instruction)\n",
    "print(\"\\nExp 7: Negative Prompting\")\n",
    "response = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': 'Describe a cat. Do NOT mention its fur color.'}\n",
    "])\n",
    "print(f\"Negative Prompt Response: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636f16e-c701-4b26-9c00-84904f8c56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 8: Temperature Tuning\n",
    "print(\"\\nExp 8: Temperature Tuning (Creative vs. Deterministic)\")\n",
    "# Lower temperature for more deterministic (factual)\n",
    "response_low_temp = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': 'Tell me a simple fact about space.'}\n",
    "], options={'temperature': 0.1})\n",
    "print(f\"\\nLow Temp Response (0.1): {response_low_temp['message']['content']}\")\n",
    "\n",
    "# Higher temperature for more creative\n",
    "response_high_temp = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': 'Write a very short, imaginative story about a talking rock.'}\n",
    "], options={'temperature': 0.8})\n",
    "print(f\"\\nHigh Temp Response (0.8): {response_high_temp['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba479362-443e-4abb-80d7-2c3928d1d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 9: Top-K/Top-P Sampling\n",
    "print(\"\\nExp 9: Top-K/Top-P Sampling\")\n",
    "# Experiment with top_k (limits vocabulary to k most likely next tokens)\n",
    "response_top_k = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': 'Complete the sentence: The sky is...'}\n",
    "], options={'top_k': 10}) # Only consider top 10 tokens\n",
    "print(f\"Top-K (10) Response: {response_top_k['message']['content']}\")\n",
    "\n",
    "# Experiment with top_p (nucleus sampling - limits to a cumulative probability)\n",
    "response_top_p = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': 'Complete the sentence: The sky is...'}\n",
    "], options={'top_p': 0.5}) # Only consider tokens that sum to 50% probability\n",
    "print(f\"Top-P (0.5) Response: {response_top_p['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606ce9a-0dab-48fe-b75c-a98382ecc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 10: System Prompt Impact\n",
    "print(\"\\nExp 10: System Prompt Impact\")\n",
    "# No system prompt\n",
    "response_no_system = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'user', 'content': 'What are your capabilities?'}\n",
    "])\n",
    "print(f\"\\nNo System Prompt: {response_no_system['message']['content']}\")\n",
    "\n",
    "# With system prompt\n",
    "response_with_system = ollama.chat(model=MODEL_NAME, messages=[\n",
    "    {'role': 'system', 'content': 'You are a highly specialized AI assistant focused only on generating rhyming poetry.'},\n",
    "    {'role': 'user', 'content': 'What are your capabilities?'} # Even this will be filtered by system prompt\n",
    "])\n",
    "print(f\"\\nWith System Prompt: {response_with_system['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c9df9-e422-49b8-8cab-0215af1dd194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
